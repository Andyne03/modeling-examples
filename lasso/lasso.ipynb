{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shrinkage Methods: The Lasso\n",
    "\n",
    "## Objective and Prerequisites\n",
    "\n",
    "In this notebook you will learn how to:\n",
    "\n",
    "1. Apply the Lasso using mathematical programming.\n",
    "2. Perform hyper-parameter tuning using random search. \n",
    "\n",
    "\n",
    "To fully understand the content of this notebook, the reader should be familiar with the following:\n",
    "\n",
    "- Differential calculus.\n",
    "- Linear algebra (matrix multiplication, transpose and inverse).\n",
    "- Linear regression analysis.\n",
    "\n",
    "---\n",
    "## Motivation\n",
    "\n",
    "For regression problems, the standard linear model is often used to describe the relationship between a response variable and a set of features. In fact, when applied to real-world problems, it is usually competitive in relation to non-linear methods. However, it may fall short when dealing with problems that have few observations with respect to the number of features considered. Particularly, and provided that the true relationship is approximately linear:\n",
    "\n",
    "- If the number of observations $n$ is much larger than the number of features $d$, i.e. $n >> d$, then the Ordinary Least Squares (OLS) estimation will tend to have low variance. Hence, the linear model will perform well on unseen data.\n",
    "- If $n$ is not much larger that $d$, then there can be a lot of variability in the fitting process, resulting in overfitting and thus a poor performance on new observations.\n",
    "- If $n < d$, there is no longer a unique solution to the OLS algorithm, as the variance is infinite.\n",
    "\n",
    "Use-cases that commonly have few observations per feature include:\n",
    "\n",
    "- Genome-scale data analysis.\n",
    "- Clinical trials.\n",
    "- Destructive testing.\n",
    "- Analysis of production systems under abnormal conditions.\n",
    "\n",
    "To exacerbate this, oftentimes some or many features may not be associated with the response, and including them would only increase the complexity of the resulting model. This notebook will present the Least Absolute Shrinkage and Selection Operator, better known as the Lasso, a technique that has gained a lot of traction because of its ability to deal with both issues.\n",
    "\n",
    "---\n",
    "## Problem Description\n",
    "\n",
    "Linear Regression is a supervised learning algorithm used to predict a quantitative response. It assumes that there is a linear relationship between the feature vector $x_i \\in \\mathbb{R}^d$ and the response $y_i \\in \\mathbb{R}$. Mathematically speaking, for sample $i$ we have $y_i = \\beta^T x_i + \\epsilon_i$, where $\\beta \\in \\mathbb{R}^d$ is the vector of feature weights, including the intercept, and  $\\epsilon_i$ is a normally-distributed random variable with zero mean and constant variance representing the error term. We can learn the weights from a training dataset with $n$ observations $\\{X \\in \\mathbb{M}^{nxd},y \\in \\mathbb{R}^n\\}$ by minimizing the Residual Sum of Squares (RSS): $e^Te =(y-X\\beta)^T (y-X\\beta)=\\beta^T X^T X\\beta- 2y^TX\\beta+y^T y$. The Ordinary Least Squares (OLS) method achieves this by taking the derivative of this quadratic and convex function and then finding the stationary point: $\\beta_{OLS}=(X^T X)^{-1} X^T y$.\n",
    "\n",
    "As previously discussed, Linear Regression does not perform well when the observations-per-feature ratio is low. Shrinkage methods, such as the Lasso, address this problem by constraining —or shrinking— the weight estimates, thus significantly reducing the variance at the expense of a slight increase in bias. \n",
    "\n",
    "Specifically, the Lasso fits a model containing all $d$ predictors, while incorporating a budget constraint based on the L1-norm of $\\beta$, disregarding the intercept component. Mathematically speaking, this method minimizes the RSS, subject to $\\sum_{l=1}^{d-1}\\mathopen|\\beta_l\\mathclose| \\leq s$, where $s$ is a hyper-parameter representing the budget that is usually tuned via cross-validation (because of this constraint, the procedure is no longer scale-invariant). This constraint has the effect of shrinking all weight estimates, also known as regularization, allowing some of them to be exactly zero when $s$ is small enough. In fact, when $s \\rightarrow 0$ the null model (intercept only) is retrieved, and when $s \\geq ||\\beta_{OLS}||_1$ the OLS model is recovered (whenever the budget constraint is non-binding). In light of this, the Lasso implicitly assumes that some of the weights truly equal to zero, allowing it to perform feature selection.\n",
    "\n",
    "It is worth noting that the unconstrained version of the Lasso is more frequently used. This version solves an unconstrained optimization problem where $RSS + \\lambda \\sum_{l=1}^{d-1}\\mathopen|\\beta_l\\mathclose|$ is minimized, for a given value of the —modified— lagrangian multiplier $\\lambda \\in \\mathbb{R}^+$. However, this notebook will focus on the model presented in the previous paragraph, which may be casted as a Quadratic Program (quadratic and convex objective function with linear constraints).\n",
    "\n",
    "---\n",
    "## Solution Approach\n",
    "\n",
    "Mathematical programming is a declarative approach where the modeler formulates a mathematical optimization model that captures the key aspects of a complex decision problem. The Gurobi optimizer solves such models using state-of-the-art mathematics and computer science. \n",
    "\n",
    "A mathematical optimization model has five components, namely:\n",
    "\n",
    "- Sets and indices.\n",
    "- Parameters.\n",
    "- Decision variables.\n",
    "- Objective function(s).\n",
    "- Constraints.\n",
    "\n",
    "Before delving into the mathematical model, Note that the budget constraint involves the L1-norm of the feature weights, which is defined as the sum of their absolute values. This constraint, in its current form, is not amenable to Quadratic Programming because it is nonlinear.  However, it is possible to linearize it using a variable transformation technique, at the expense of additional decision variables.\n",
    "\n",
    "Let $\\beta_j = \\beta_j^+ - \\beta_j^-$ and $|\\beta_j| = \\beta_j^+ + \\beta_j^-$, where $\\beta_j^+, \\beta_j^- \\geq 0 \\quad \\forall j \\in \\{1,2,\\dots, d\\}$ (for brevity, the intercept is also transformed). Then, the budget constraint can be re-expressed as $\\sum_{l=1}^{d-1}(\\beta_l^+ + \\beta_l^-) \\leq s$. Consequently, the RSS is modified as follows:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "e^Te \\\\\n",
    "=(y^T-\\beta^TX)(y-X\\beta) \\\\\n",
    "=[y^T-(\\beta^+-\\beta^-)^TX^T][y-X(\\beta^+-\\beta^-)] \\\\\n",
    "=y^Ty -y^TX\\beta^+ +y^TX\\beta^- -\\beta^{+T}X^Ty +\\beta^{+T}X^TX\\beta^+ -\\beta^{+T}X^TX\\beta^- +\\beta^{-T}X^Ty -\\beta^{-T}X^TX\\beta^+ +\\beta^{-T}X^TX\\beta^- \\\\\n",
    "= \\beta^{+T}X^TX\\beta^+ +\\beta^{-T}X^TX\\beta^- -2\\beta^{+T}X^TX\\beta^- -2y^TX\\beta^+ +2y^TX\\beta^- +y^Ty\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "As a side note, it can be shown from the definitions that $\\beta_j^+ = \\frac{|\\beta_j|+\\beta_j}{2}$ and $\\beta_j^- = \\frac{|\\beta_j|-\\beta_j}{2}$, which in turn implies that $\\beta_j^-=0$ when $\\beta_j \\geq 0$ and $\\beta_j^+=0$ when $\\beta_j \\leq 0$. In other words, $\\beta_j^+ \\cdot \\beta_j^- = 0 \\quad \\forall j \\in \\{1,2,\\dots, d\\}$.\n",
    "\n",
    "We are now ready to present a QP formulation that finds the weight estimates for a linear regression problem with an L1-norm constraint on the feature weights:\n",
    "\n",
    "### Sets and Indices\n",
    "$i \\in I=\\{1,2,\\dots,n\\}$: Set of observations.\n",
    "\n",
    "$j \\in J=\\{1,2,\\dots,d\\}$: Set of features, where the last ID corresponds to the intercept.\n",
    "\n",
    "$l \\in L = J \\backslash \\{d\\}$: Set of features, where the intercept is excluded.\n",
    "\n",
    "### Parameters\n",
    "$s \\in \\mathbb{R}^+$: Budget available for the L1-norm of $\\beta$.\n",
    "\n",
    "$Q = X^T X \\in \\mathbb{M}^{|J|x|J|}$: Quadratic component of the objective function.\n",
    "\n",
    "$c = y^TX \\in \\mathbb{R}^{|J|}$: Linear component of the objective function.\n",
    "\n",
    "***Note:** Recall that the RSS is defined as $\\beta^{+T}X^TX\\beta^+ +\\beta^{-T}X^TX\\beta^- -2\\beta^{+T}X^TX\\beta^- -2y^TX\\beta^+ +2y^TX\\beta^- +y^Ty$. However, Since $y^T y$ is constant w.r.t. the decision variables, we can drop this term altogether.\n",
    "\n",
    "### Decision Variables\n",
    "$\\beta_j = \\beta_j^+ - \\beta_j^- \\in \\mathbb{R}$: Weight of feature $j$, representing the change in the response variable per unit-change of feature $j$.\n",
    "\n",
    "$\\beta_j^+, \\beta_j^- \\in \\mathbb{R}^+$: Auxiliary variables used to model the absolute value of $\\beta_j$.\n",
    "\n",
    "### Objective Function\n",
    "\n",
    "- **Training error**: Minimize the Residual Sum of Squares (RSS):\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Min} \\quad Z = \\frac{1}{2}\\sum_{j \\in J}\\sum_{j' \\in J}Q_{j,j'}\\beta_j^+\\beta_{j'}^+ +\\frac{1}{2}\\sum_{j \\in J}\\sum_{j' \\in J}Q_{j,j'}\\beta_j^-\\beta_{j'}^- -\\sum_{j \\in J}\\sum_{j' \\in J}Q_{j,j'}\\beta_j^+\\beta_{j'}^- - \\sum_{j \\in J}c_j\\beta_j^+ + \\sum_{j \\in J}c_j\\beta_j^-\n",
    "\\tag{0}\n",
    "\\end{equation}\n",
    "\n",
    "Note that we use the fact that if $x^*$ is a minimizer of $f(x)$, it is also a minimizer of $a\\cdot f(x)$, as long as $a > 0$.\n",
    "\n",
    "### Constraints\n",
    "\n",
    "- **Budget constraint**: The L1-norm of the feature weights cannot exceed the budget:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{l \\in L}(\\beta_l^+ + \\beta_l^-) \\leq s\n",
    "\\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "---\n",
    "## Python Implementation\n",
    "\n",
    "In the following implementation, we make use of three main libraries:\n",
    "\n",
    "- **Numpy** for scientific computing.\n",
    "- **Scikit learn** for machine learning algorithms.\n",
    "- **Gurobi** for mathematical optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import all the necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from gurobipy import *\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error as mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and deploy the optimization model\n",
    "\n",
    "def lasso(X, y, budget, verbose=False):\n",
    "    \"\"\"\n",
    "    Deploy and optimize the QP formulation of the Lasso. It assumes that data is\n",
    "    centered and scaled.\n",
    "    \"\"\"\n",
    "    regressor = Model()\n",
    "    samples, dim = X.shape\n",
    "    assert samples == y.shape[0]  \n",
    "    assert budget >= 0\n",
    "    \n",
    "    # Append a column of ones to the feature matrix to account for the y-intercept\n",
    "    X = np.concatenate([X, np.ones((samples, 1))], axis=1)\n",
    "    \n",
    "    # Decision variables. Note that a change of variable is used\n",
    "    beta_plus = regressor.addVars(dim + 1, name=\"beta_plus\") # Weight estimates\n",
    "    beta_minus = regressor.addVars(dim + 1, name=\"beta_minus\") # Weight estimates\n",
    "    intercept_plus = beta_plus[dim] # Last decision variable captures the y-intercept\n",
    "    intercept_minus = beta_minus[dim] # Last decision variable captures the y-intercept\n",
    "    intercept_plus.varname = 'intercept_plus'\n",
    "    intercept_minus.varname = 'intercept_minus'\n",
    "    \n",
    "    # Objective Function (OF): minimize 1/2 * RSS using the facts that\n",
    "    # if x* is a minimizer of f(x), it is also a minimizer of k*f(x) iff k > 0\n",
    "    # if x* is a minimizer of f(x), it is also a minimizer of f(x) + k\n",
    "    Quad = np.dot(X.T, X)\n",
    "    lin = np.dot(y.T, X)\n",
    "    obj = sum(0.5 * Quad[i,j] * beta_plus[i] * beta_plus[j] for i, j in product(range(dim + 1), repeat=2))\n",
    "    obj += sum(0.5 * Quad[i,j] * beta_minus[i] * beta_minus[j] for i, j in product(range(dim + 1), repeat=2))\n",
    "    obj -= sum(Quad[i,j] * beta_plus[i] * beta_minus[j] for i, j in product(range(dim + 1), repeat=2))\n",
    "    obj -= sum(lin[i] * beta_plus[i] for i in range(dim + 1))\n",
    "    obj += sum(lin[i] * beta_minus[i] for i in range(dim + 1))\n",
    "    regressor.setObjective(obj, GRB.MINIMIZE)\n",
    "    \n",
    "    # Budget constraint. Note that the intercept is not included\n",
    "    regressor.addConstr(quicksum([beta_plus[j] + beta_minus[j] for j in range(dim)]) == budget)\n",
    "    \n",
    "    if not verbose:\n",
    "        regressor.params.OutputFlag = 0\n",
    "    regressor.params.timelimit = 60\n",
    "    regressor.optimize()\n",
    "    \n",
    "    coeff_plus = np.array([beta_plus[i].X for i in range(dim)])\n",
    "    coeff_minus = np.array([beta_minus[i].X for i in range(dim)])\n",
    "    return intercept_plus.X - intercept_minus.X, coeff_plus - coeff_minus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Hyper-parameter Tuning\n",
    "\n",
    "Unlike Ordinary Least Squares (OLS), the Lasso produces a different set of weight estimates $\\beta(s)$ for each value of the budget $s$. Thus, we require a method for selecting a value for that hyper-parameter. As $s \\in \\mathbb{R}^+$, we will:\n",
    "\n",
    "1. Try several values using random search and compute the cross-validated Mean Square Error (MSE) for each of them.\n",
    "2. Select the budget for which the cross-validated MSE is smallest.\n",
    "3. Fit a model using all of the available observations and the selected value for the budget.\n",
    "\n",
    "***Note:** As previously stated, we can bound our search to $0 \\leq s \\leq ||\\beta_{OLS}||_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_folds(features, response, train_mask):\n",
    "    \"\"\"\n",
    "    Assign folds to either train or test partitions based on train_mask.\n",
    "    \"\"\"\n",
    "    xtrain = features[train_mask,:]\n",
    "    xtest = features[~train_mask,:]\n",
    "    ytrain = response[train_mask]\n",
    "    ytest = response[~train_mask]\n",
    "    return xtrain, xtest, ytrain, ytest\n",
    "\n",
    "def cross_validate(features, response, budget, folds, seed):\n",
    "    \"\"\"\n",
    "    Train a Lasso model for each fold and report the cross-validated MSE.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    samples, dim = features.shape\n",
    "    assert samples == response.shape[0]\n",
    "    fold_size = int(np.ceil(samples / folds))\n",
    "    # Randomly assign each sample to a fold\n",
    "    shuffled = np.random.choice(samples, samples, replace=False)\n",
    "    mse_cv = 0\n",
    "    # Exclude folds from training, one at a time, to get out-of-sample estimates of the MSE\n",
    "    for fold in range(folds):\n",
    "        idx = shuffled[fold * fold_size : min((fold + 1) * fold_size, samples)]\n",
    "        train_mask = np.ones(samples, dtype=bool)\n",
    "        train_mask[idx] = False\n",
    "        xtrain, xtest, ytrain, ytest = split_folds(features, response, train_mask)\n",
    "        intercept, beta = lasso(xtrain, ytrain, budget)\n",
    "        ypred = np.dot(xtest, beta) + intercept\n",
    "        mse_cv += mse(ytest, ypred) / folds\n",
    "    # Report the average out-of-sample MSE\n",
    "    return mse_cv\n",
    "\n",
    "def get_budget_UB(X, y):\n",
    "    \"\"\"\n",
    "    Calculates the L1 norm of the OLS estimates, in log10 units.\n",
    "    \"\"\"\n",
    "    X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "    arg1 = np.linalg.inv(np.dot(X.T, X))\n",
    "    arg2 = np.dot(X.T, y)\n",
    "    budget = np.linalg.norm(np.dot(arg1, arg2)[:-1], ord=1)\n",
    "    return np.log10(budget)\n",
    "\n",
    "def lasso_cv(features, response, trials=10, folds=5, seed=None):\n",
    "    \"\"\"\n",
    "    Looks for a promising value for the budget on a log-linear space and\n",
    "    returns the weight estimates for the corresponding Lasso model.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    upper_bound = get_budget_UB(features, response)\n",
    "    best_mse = np.inf\n",
    "    best_budget = None\n",
    "    for i in range(trials):\n",
    "        # Explore values between the null and OLS models, in log-linear space\n",
    "        exponent = np.random.uniform(0, upper_bound)\n",
    "        budget = np.power(10, exponent)\n",
    "        val = cross_validate(features, response, budget, folds=folds, seed=seed)\n",
    "        if val < best_mse:\n",
    "            best_mse = val\n",
    "            best_budget = budget\n",
    "    intercept, beta = lasso(features, response, best_budget)\n",
    "    return intercept, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Benchmark\n",
    "\n",
    "We will validate the output of the implementation presented above with the one provided by Scikit learn. The Boston dataset is used for this purpose. This dataset measures the prices of 506 houses, along with 13 features that provide insights about their neighbourhoods. We will use the original feature terminology, so the interested reader is referred to [this website](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html). \n",
    "\n",
    "Note that 20% of the samples are reserved for computing the out-of-sample MSE. Furthermore, as the Lasso is not scale-invariant, we first standardize all features so they are expressed in the same units. Such preprocessing entails three steps, namely:\n",
    "\n",
    "For each feature $x_l$:\n",
    "1. Compute its sample average $\\mu_l$ and sample standard deviation $\\sigma_l$.\n",
    "2. Center by subtracting $\\mu_l$ from $x_l$.\n",
    "3. Scale by dividing the resulting difference by $\\sigma_l$. \n",
    "\n",
    "This procedure has the effect of coercing the sample average and variance to be equal to zero and one, respectively, across all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and split into train (80%) and test (20%)\n",
    "boston = load_boston()\n",
    "X = boston['data']\n",
    "y = boston['target']\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.20, random_state=10101)\n",
    "# Center and scale data to have zero mean and variance of one\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(Xtrain)\n",
    "Xtrain = scaler.transform(Xtrain)\n",
    "Xtest = scaler.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.36210006159952"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyper-parameter tuning using random search\n",
    "b0, beta = lasso_cv(Xtrain, ytrain, seed=10101)\n",
    "mse(ytest, np.dot(Xtest, beta) + b0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.36501227078064"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyper-parameter tuning performed using grid search:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html\n",
    "lr = linear_model.LassoCV(cv=5)\n",
    "lr.fit(Xtrain, ytrain)\n",
    "mse(ytest, lr.predict(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The L1-norm of Qp's output is: 22.1\n",
      "The L1-norm of Scikit-learn's output is: 22.14\n"
     ]
    }
   ],
   "source": [
    "# intercept is not accounted for\n",
    "norm_qp = np.round(np.linalg.norm(np.array(beta), 1), 2)\n",
    "norm_sklearn = np.round(np.linalg.norm(lr.coef_, 1), 2)\n",
    "print(\"The L1-norm of Qp's output is: {0}\".format(norm_qp))\n",
    "print(\"The L1-norm of Scikit-learn's output is: {0}\".format(norm_sklearn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that both metrics, the MSE and the L1-norm of the vector of feature weights, are virtually the same in both implementations. Notice that the norms are not exactly equal because the implementation in Scikit-learn uses grid search, rather than random search.\n",
    "\n",
    "### Final Model\n",
    "\n",
    "The previous analysis indicates that the best model is as follows:\n",
    "\n",
    "$$\n",
    "\\text{medv} = 22.56-1.00\\text{crim}+1.41\\text{zn}+0.49\\text{chas}-1.84\\text{nox}+2.57\\text{rm}\n",
    "$$\n",
    "\n",
    "$$\n",
    "-1.84\\text{age}-3.48\\text{dis}+2.52\\text{rad}-2.12\\text{tax}-1.85\\text{ptratio}+1.01\\text{b}-3.64\\text{lstat}\n",
    "$$\n",
    "\n",
    "Since we standardized the data, the intercept represents the estimated median value (in thousands) of a house with mean values across features. Likewise, we can interpret $\\beta_1=-1.00$ as the decrease in the house value when the per-capita crime rate increases by one standard deviation from the average value, *ceteris paribus* (similar statements can be made for the rest of the features).\n",
    "\n",
    "---\n",
    "## Conclusion\n",
    "\n",
    "As already shown, the Lasso is especially useful when we have few observations for the learning process, or when we want to increase the interpretability of the model.\n",
    "\n",
    "Also, this notebook presented a mathematical model based on Quadratic Programming to find an optimal solution for the Lasso. While this approach may not be the fastest, it can easily accommodate additional linear constraints (Bertsimas, 2015), such as: \n",
    "\n",
    "- Enforcing group sparcity among features.\n",
    "- Limiting pairwise multicollinearity.\n",
    "- Limiting global multicollinearity.\n",
    "- Considering a fixed set of nonlinear transformations.\n",
    "\n",
    "Thus, mathematical programming provides a systematic approach to address common aspects of the model-building process, such as imposing statistical properties, which otherwise would be hard to incorporate into specialized algorithms. Ultimately, it comes down to choosing between efficiency and versatility.\n",
    "\n",
    "---\n",
    "## References\n",
    "\n",
    "1. Bertsimas, D., & King, A. (2015). OR forum—An algorithmic approach to linear regression. Operations Research, 64(1), 2-16.\n",
    "2. Busa, J. (2012). Solving quadratic programming problem with linear constraints containing absolute values. Acta Electrotechnica et Informatica, 12(3), 11.\n",
    "3. James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning. New York: springer.\n",
    "4. The Boston housing dataset (1996, October 10). Retrieved from https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html\n",
    "5. Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Methodological), 58(1), 267-288."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright &copy; 2019 Gurobi Optimization, LLC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
